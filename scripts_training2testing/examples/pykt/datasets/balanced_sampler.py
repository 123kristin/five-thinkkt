#!/usr/bin/env python3
"""
å¹³è¡¡é‡‡æ ·å™¨ï¼Œç¡®ä¿æ¯ä¸ªbatchåŒ…å«æ‰€æœ‰éš¾åº¦ç±»åˆ«
"""

import torch
from torch.utils.data import Sampler
import numpy as np
from collections import defaultdict


class DifficultyBalancedSampler(Sampler):
    """
    éš¾åº¦å¹³è¡¡é‡‡æ ·å™¨
    ç¡®ä¿æ¯ä¸ªbatchéƒ½åŒ…å«æ‰€æœ‰difficultyç±»åˆ«çš„æ ·æœ¬
    """
    
    def __init__(self, dataset, batch_size, num_difficulty_classes=3, min_samples_per_class=2):
        """
        Args:
            dataset: CzyKTDatasetå®ä¾‹
            batch_size: batchå¤§å° 
            num_difficulty_classes: éš¾åº¦ç±»åˆ«æ•°é‡ (0,1,2)
            min_samples_per_class: æ¯ä¸ªç±»åˆ«åœ¨batchä¸­çš„æœ€å°æ ·æœ¬æ•°
        """
        self.dataset = dataset
        self.batch_size = batch_size
        self.num_difficulty_classes = num_difficulty_classes
        self.min_samples_per_class = min_samples_per_class
        
        # æŒ‰éš¾åº¦ç±»åˆ«åˆ†ç»„æ ·æœ¬ç´¢å¼•
        self.difficulty_to_indices = self._group_by_difficulty()
        
        # è®¡ç®—æ¯ä¸ªéš¾åº¦ç±»åˆ«çš„æ ·æœ¬æ•°
        self.class_counts = {diff: len(indices) for diff, indices in self.difficulty_to_indices.items()}
        print(f"ğŸ“Š BalancedSampleréš¾åº¦åˆ†å¸ƒ: {self.class_counts}")
        
        # è®¡ç®—æ€»çš„batchæ•°é‡
        total_samples = len(dataset)
        self.num_batches = total_samples // batch_size
        
    def _group_by_difficulty(self):
        """æŒ‰éš¾åº¦ç±»åˆ«åˆ†ç»„æ ·æœ¬ç´¢å¼•"""
        difficulty_to_indices = defaultdict(list)
        
        for idx in range(len(self.dataset)):
            try:
                sample = self.dataset[idx]
                if 'qdseqs' in sample:
                    # è·å–æ ·æœ¬çš„èšåˆéš¾åº¦
                    qdseqs = sample['qdseqs']
                    valid_difficulties = qdseqs[qdseqs != -1]
                    if len(valid_difficulties) > 0:
                        # ä½¿ç”¨ç›¸åŒçš„èšåˆç­–ç•¥
                        mean_diff = valid_difficulties.float().mean().round().long().clamp(0, 2).item()
                        difficulty_to_indices[mean_diff].append(idx)
                    else:
                        difficulty_to_indices[0].append(idx)  # é»˜è®¤ä¸ºeasy
                else:
                    difficulty_to_indices[0].append(idx)  # é»˜è®¤ä¸ºeasy
            except:
                difficulty_to_indices[0].append(idx)  # å‡ºé”™æ—¶é»˜è®¤ä¸ºeasy
        
        return difficulty_to_indices
    
    def __iter__(self):
        """ç”Ÿæˆå¹³è¡¡çš„batch"""
        # ä¸ºæ¯ä¸ªéš¾åº¦ç±»åˆ«åˆ›å»ºå¾ªç¯è¿­ä»£å™¨
        iterators = {}
        for diff in range(self.num_difficulty_classes):
            if diff in self.difficulty_to_indices and len(self.difficulty_to_indices[diff]) > 0:
                indices = self.difficulty_to_indices[diff].copy()
                np.random.shuffle(indices)  # éšæœºæ‰“ä¹±
                iterators[diff] = self._cycle_iterator(indices)
            else:
                # å¦‚æœæŸä¸ªéš¾åº¦ç±»åˆ«æ²¡æœ‰æ ·æœ¬ï¼Œç”¨å…¶ä»–ç±»åˆ«ä»£æ›¿
                print(f"âš ï¸ è­¦å‘Šï¼šéš¾åº¦ç±»åˆ«{diff}æ²¡æœ‰æ ·æœ¬ï¼Œå°†ç”¨å…¶ä»–ç±»åˆ«ä»£æ›¿")
        
        # ç”Ÿæˆbalanced batches
        all_batch_indices = []
        
        for batch_idx in range(self.num_batches):
            batch_indices = []
            
            # æ¯ä¸ªéš¾åº¦ç±»åˆ«è‡³å°‘åŒ…å«min_samples_per_classä¸ªæ ·æœ¬
            for diff in range(self.num_difficulty_classes):
                if diff in iterators:
                    for _ in range(self.min_samples_per_class):
                        if len(batch_indices) < self.batch_size:
                            batch_indices.append(next(iterators[diff]))
            
            # å‰©ä½™ä½ç½®éšæœºå¡«å……
            remaining_slots = self.batch_size - len(batch_indices)
            if remaining_slots > 0:
                # æŒ‰åŸå§‹åˆ†å¸ƒæ¯”ä¾‹å¡«å……å‰©ä½™ä½ç½®
                available_diffs = list(iterators.keys())
                for _ in range(remaining_slots):
                    # éšæœºé€‰æ‹©ä¸€ä¸ªéš¾åº¦ç±»åˆ«
                    diff = np.random.choice(available_diffs)
                    batch_indices.append(next(iterators[diff]))
            
            # éšæœºæ‰“ä¹±batchå†…çš„é¡ºåº
            np.random.shuffle(batch_indices)
            all_batch_indices.extend(batch_indices)
        
        return iter(all_batch_indices)
    
    def _cycle_iterator(self, indices):
        """åˆ›å»ºå¾ªç¯è¿­ä»£å™¨ï¼Œè€—å°½åé‡æ–°å¼€å§‹"""
        while True:
            for idx in indices:
                yield idx
            # é‡æ–°æ‰“ä¹±é¡ºåº
            np.random.shuffle(indices)
    
    def __len__(self):
        """è¿”å›æ€»æ ·æœ¬æ•°"""
        return self.num_batches * self.batch_size


def create_balanced_dataloader(dataset, batch_size=256, num_workers=0, **kwargs):
    """
    åˆ›å»ºä½¿ç”¨å¹³è¡¡é‡‡æ ·å™¨çš„DataLoader
    
    Args:
        dataset: CzyKTDatasetå®ä¾‹
        batch_size: batchå¤§å°
        num_workers: å·¥ä½œçº¿ç¨‹æ•°
        **kwargs: å…¶ä»–DataLoaderå‚æ•°
    
    Returns:
        torch.utils.data.DataLoader
    """
    from torch.utils.data import DataLoader
    
    # åˆ›å»ºå¹³è¡¡é‡‡æ ·å™¨
    sampler = DifficultyBalancedSampler(dataset, batch_size)
    
    # åˆ›å»ºDataLoader
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        num_workers=num_workers,
        **kwargs
    )
    
    return dataloader 