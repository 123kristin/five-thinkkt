基本版 ThinkKT 流程：

    1. 【千问大模型（Qwen2.5-VL）】
    题目图片 + Prompt 
    → 提取视觉特征 
    → v_t: 题目表征 (d_question=1024)

    2. 【知识点分类器（kc_classifier）】
    v_t (题目表征)
    → 预测知识点分布
    → k_t: 知识点分布 (num_c维度，每个知识点0-1概率)

    3. 【答案嵌入（answer_emb）】
    rseqs (答题结果: 0或1)
    → Embedding层
    → a_emb: 答案表征 (d_answer维度)

    4. 【特征融合】
    z = [v_t, a_emb, k_t]  # 拼接
    → fusion_layer (Linear + LayerNorm + ReLU + Dropout)
    → z (d_knowledge=512)

    5. 【序列建模】
    z
    → LSTM 或 Transformer（带因果掩码）
    → h_t (知识状态表示)

    6. 【预测】
    h_t
    → predictor (Linear + Sigmoid)
    → y: 预测下一道题目答题正确的概率

ThinkKT CoT 版本流程
    1. 【CoT生成（CoTGenerator）】
    输入：
    ├─ 题目图片
    ├─ 历史交互序列（history_qids, history_rs）
    └─ 知识点信息（可选，用于构建Prompt）
    
    → 构建Prompt（包含历史交互信息和当前题目）
    → 千问大模型（Qwen2.5-VL）
    → CoT文本

    2. 【文本编码（encode_cot）】
    CoT文本
    → Sentence-Transformers（文本嵌入模型）
    → r_embed: CoT表征 (d_cot=384)

    3. 【特征融合（ThinkKTNet）】
    z = [v_t, a_emb, r_embed, k_t]
    ├─ v_t: 题目特征（从图片提取的视觉特征）
    ├─ a_emb: 答案表征（答题结果嵌入）
    ├─ r_embed: CoT表征 ⭐（新增）
    └─ k_t: 知识点分布
    
    → fusion_layer
    → z (d_knowledge=512)

    4. 【序列建模】
    z → LSTM 或 Transformer（带因果掩码）
    → h_t

    5. 【预测】
    h_t → predictor
    → y: 预测下一道题目答题正确的概率

ThinkKT 强化学习版本

    ┌─────────────────────────────────────────────────────────────────┐
    │                    ThinkKT RL 版本训练流程                        │
    └─────────────────────────────────────────────────────────────────┘

    1. 【数据输入】
    ├─ qseqs: 问题ID序列 (batch, seq_len)
    ├─ rseqs: 历史答题结果序列 (batch, seq_len)
    ├─ shft_rseqs: 真实标签（下一题答题结果）(batch, seq_len)
    ├─ cseqs: 知识点序列 (batch, seq_len, max_concepts)
    ├─ img_path_dict: 题目图片路径映射
    └─ kc_vocab: 知识点词表

    2. 【CoT生成】（使用当前策略）
    CoTGenerator (可训练参数)
    ├─ 输入：题目图片 + 历史交互 + 知识点信息
    ├─ 千问大模型 → CoT文本
    └─ 文本编码器 → CoT嵌入 (batch, seq_len, d_cot)

    3. 【KT模型预测】
    
    3a. 不使用CoT的预测（基线）
        ThinkKT (use_cot=False)
        └─ y_no_cot: (batch, seq_len)
    
    3b. 使用CoT的预测
        ThinkKT (use_cot=True, 输入生成的CoT嵌入)
        └─ y_with_cot: (batch, seq_len)

    4. 【奖励计算】R = R_pred + R_cons + R_kc + R_len
    
    4a. R_pred: 预测准确性奖励
        = (BCE_loss(无CoT) - BCE_loss(有CoT)) × 1.0
        → 鼓励CoT提升预测准确性
    
    4b. R_cons: 一致性奖励
        = consistency_score × 0.5
        → 鼓励CoT中的"掌握/薄弱"描述与预测概率一致
        - CoT说"掌握" → 预测概率应该高
        - CoT说"薄弱" → 预测概率应该低
    
    4c. R_kc: 知识点覆盖奖励
        = coverage_score × 0.3
        → 鼓励CoT提及题目的知识点
        - 计算CoT中提到的知识点 / 题目总知识点数
    
    4d. R_len: 长度惩罚
        = length_penalty × 0.1
        → 鼓励CoT长度在80-120 tokens之间
        - 太短或太长都会受到惩罚

    5. 【策略梯度优化】（REINFORCE算法）
    
    5a. 计算优势（Advantage）
        advantages = rewards - rewards.mean()
        → 减去基线，减少方差
    
    5b. 计算损失
        loss = -(log_probs × advantages).mean()
        → 最大化（奖励 × 动作概率）
    
    5c. 反向传播
        optimizer.zero_grad()
        loss.backward()
        clip_grad_norm_(max_norm=1.0)
        optimizer.step()
        → 更新CoT生成器参数

    6. 【输出指标】
    ├─ reward_mean: 平均奖励
    ├─ reward_std: 奖励标准差
    ├─ loss: RL损失值
    └─ advantage_mean: 平均优势值

改进1：
    我的提示词：
        当前thinkkt模型基本版本的输入融合为“
        v_t：题目特征（视觉特征）
        a_emb：答案嵌入（0/1 → 嵌入向量）
        k_t：知识点分布（从视觉特征预测，shape: (batch, seq_len, num_c)）
        注意：k_t 是由 visual_encoder 从图片预测的知识点分布，不是直接输入的知识点标签。” 
        而crkt入为“具体结构：
        习题嵌入：q_emb = self.QEmbs(q)
        知识点平均嵌入：kc_avg_embs = self.get_kc_avg_emb(c)（对多个知识点求平均）
        拼接：input_components = [q_emb, content_emb(可选), kc_avg_embs]
        答对/答错编码：
        答对时：[q_emb, content_emb(可选), kc_avg_embs, zero_vector]
        答错时：[zero_vector, q_emb, content_emb(可选), kc_avg_embs]
        通过零向量的位置来编码答对/答错，而不是直接拼接答案信息。最终输入是 e_emb，传给 RNN/LSTM。”
        我的需求是将thinkkt模型的输入改为参照crkt模型的做法，但是习题嵌入用我们的“v_t：题目特征（视觉特征）”，然后不适用生成知识点分布，而是使用类似crkt使用的“知识点平均嵌入：kc_avg_embs “，答对答错时也参照crkt模型使用的零向量左右拼接来区分。你明白我的需求吗？你先别实现代码，你先简洁回答我，需要做的事情  @thinkkt_net.py @crkt.py 

分析“
    需要做的事：
        移除答案嵌入（a_emb）：不再使用 answer_emb 将 0/1 转为嵌入向量
        移除预测的知识点分布（k_t）：不再从视觉特征预测 k_t，改为使用输入的知识点标签
        添加知识点平均嵌入计算：类似 CRKT 的 get_kc_avg_emb，从输入的 c（知识点序列）计算 kc_avg_embs
        修改输入融合方式：
        答对时（r==1）：[v_t, kc_avg_embs, zero_vector]
        答错时（r==0）：[zero_vector, v_t, kc_avg_embs]
        通过零向量位置编码答对/答错
        调整融合层输入维度：从 d_question + d_answer + num_c 改为 d_question + dim_qc + dim_qc*2（v_t + kc_avg_embs + zero_vector）
        修改 ThinkKT 主模型：移除 k_t 的生成，直接传入 c 给 ThinkKTNet 计算知识点平均嵌入
        总结：用知识点标签的平均嵌入替代预测的知识点分布，用零向量位置替代答案嵌入，其他保持不变

对比：
    ThinkKT：
    输入融合 → fusion_layer（Linear + LayerNorm + ReLU + Dropout）
    序列建模 → LSTM/Transformer
    预测头 → predictor（Linear + ReLU + Dropout + Linear + Sigmoid）直接输出概率
    CRKT：
    输入融合 → 直接拼接（无 fusion_layer）
    序列建模 → LSTM/GRU
    预测所有问题分数 → q_scores_extractor（Dropout + Linear）输出 (batch, seq_len, num_q)
    选择对应问题 → 通过 one_hot(q_shift) 与 q_scores 点积，得到该问题的预测概率
    关键区别：
    ThinkKT：先融合降维，再序列建模，最后直接预测概率
    CRKT：直接序列建模，预测所有问题的分数矩阵，再通过 one_hot 选择对应问题

保存在：
    five-thinkkt/scripts_training2testing/examples/saved_model/baseline_version_input这是基本版本，但是输入已经改成类似统一做法，使用题目表征与知识点平均表征，打哪向量左右拼接区分对错
    five-thinkkt/scripts_training2testing/examples/saved_model/cot_version_input这是CoT版本，生成思维链和思维链表征,DBE_KT22和nips_task34数据集是英文的，思维链就该是英文的，XES3G5M是中文的，生成的思维链是中文的
实验发现使用CoT参数为1时，生成的思维链缓存为batch_size×(seqlen-1)，时间太久

改进2：采用“高重叠不生成”策略（即只对新出现的或生疏的知识点生成 CoT）：
    在这 20 道三角函数题中，可能只有第 1 道（全新出现）或者前几道会触发 CoT 生成。
    剩下的十几道题，因为知识点已经在历史窗口中频繁出现了，就会触发“跳过”逻辑。
    (我本来的思想策略：仅当当前题目的知识点在历史记录中出现过（即有重叠）时，才生成 CoT；如果是全新的知识点，就不生成。
    潜在风险
    这可能恰恰把最需要 CoT 的情况给过滤掉了。
    冷启动问题（Cold Start）： 当学生第一次遇到一个新知识点时，模型是最无助的。因为它没有任何该学生关于该知识点的历史数据（History = 0）。
    常规 KT 模型/Baseline：这时候只能瞎猜（通常给一个类平均得分），因为它没见学生做过这类题。
    ThinkKT 的独特优势：CoT 的最大价值就在这里！即使没有历史记录，LLM 还可以通过读题（看图、看题干逻辑）来进行零样本推理 (Zero-shot Reasoning)。
    LLM 可以分析：“这道题虽然没见过，但根据图里的几何关系，这应该这样做...” 或者 “虽然没见过这个知识点，但这道题的常识逻辑是...”
    结论：如果你在这个时候（无历史重叠）把 CoT 关了，就相当于把 LLM 最强大的“救急”能力给封印了，退化回了一个普通的基线模型。
    推理的依赖性： CoT 的 Prompt 通常包含两部分：
    Part A：基于历史表现回顾（“你以前做错过这类题...”）
    Part B：基于当前题目分析（“这道题的难点在于...”）
    如果历史没有重叠，Part A 确实没用；但 Part B 依然极具价值。
    反向建议
    它建议反过来思考：

    如果当前知识点在历史中频繁出现（比如做了 50 道一元一次方程），模型已经摸透了这个学生的水平，这时候反而不需要 CoT，因为统计规律已经足够强了。
    如果当前知识点从未出现过（新知识点），或者只出现过一两次，这时候统计规律不可靠，最需要 CoT 来提供额外的题目语义分析辅助。
    所以，与其“无重叠不生成”，不如**“高重叠不生成”**（即只对生疏或全新的知识点生成）。
    通过阈值设定为2来控制)
改进3：
发现输出预测头不对，把基础班thinkkt模型改成与CRKT类似的框架，除了输入的交互信息融合处使用的是经过千问大模型获得的题目表征，其他度一样

2025-12-23
run_parallel_datasets.sh运行实验：
CRKT基线版本的thinkkt，除了一个线性层，其他与CRKT一致，lstm层数可选1,2,3
视觉基线版本的thinkkt，用大模型获得的题目表征替换了CRKT版本中的题目id编码，lstm层数可选1,2,3
以上两组实验并行
保存在saved_model/direct_concat/crkt_baseline中
run_rule_cot_experiments.sh运行实验：
CRKT+CoT版本的thinkkt，加上基于规则策略的思维链，即设置阈值为2，lstm层数可选1,2,3
视觉基线版本的thinkkt，加上基于规则策略的思维链，即设置阈值为2，lstm层数可选1,2,3
以上实验串行，因为并行会导致生成的思维链缓存冲突
保存在saved_model/crkt_cot_rule中
现在需要做的事是：
等所有思维链缓存生成完毕，看模型性能，以及添加强化学习，再看模型性能
bash run_parallel_datasets.sh完成后，
crkt_baseline和visual_baseline移动到direct_concat文件夹下保存
而crkt_cot_rule则继续生成思维链供后续使用

下午：
实验结果发现，由于添加了这个线性融合层，DBE的性能提升，但是XES性能降低挺多的，不到0.8的AUC甚至
还由于直接将1024维的大模型提取的题目表征降维至200再拼接，也有可能导致XES性能下降
所以修改：
原本id表征的d_question维度改回200，确保完全复刻CRKT；
将大模型获得的题目表征先降维至200，再对题目id表征替换；
将大模型获得的题目表征先降维至200，再和题目id表征拼接，而不是替换；
这里就需要将参数--question_rep_type设置为["visual", "qid", "v&q"]；
代码适配;
然后写一个脚本用来运行以下实验：
3个数据集，3个显卡，CRKT复刻版本thinkkt，视觉替换版本thinkkt，视觉拼接版本thinkkt，lstm层数可选1，2,3；
日志保存到bs_logs文件夹下，训练最佳模型保存到bs_qid,bs_visual,bs_v&q文件夹下
先不管思维链和强化学习的实验。
git add .
git commit -m "true bs"
git push
bash run_bs_experiments.sh

晚上：
先等run_rule_cot_experiments.sh运行完毕，才能生成思维链缓存
思维链如何生成？
    1.基于规则策略，即设定阈值2
    2.基于元选择什么的策略
思维链如何利用到thinkkt模型中？
1.直接从384降维至200与输入拼接
2.CoT 引导的知识门控 (CoT-Guided Knowledge Gate)
    具体设计：
    在 LSTM 内部更新公式中，引入一个由 CoT 动态生成的 “推理置信度门 (Reasoning Gate)” $g_{cot}$。

    计算门控信号： $$g_{cot} = \sigma(W_g \cdot \text{Projector}(CoT_t) + b_g)$$
    这个 $g_{cot}$ (范围 0~1) 代表了模型认为当前这道题的推理逻辑提供了多大的“信息增益”。
    调节状态更新： 传统的 LSTM 遗忘门/输入门大致是基于 $x_t$ 和 $h_{t-1}$ 计算的。 我们可以显式地用 $g_{cot}$ 去增强输入门 (Input Gate)： $$i_t' = i_t \cdot (1 + \lambda \cdot g_{cot})$$
    直观解释：如果 CoT 质量很高（逻辑清晰），$g_{cot}$ 较大，Input Gate 被放大，不仅这道题的信息被更多地写入知识状态，而且模型“更确信”这次知识点的掌握情况发生了变化。